{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "611bbe27-6e93-4272-b99f-e82540d1f46a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salvo il file in csv THEWITCHER.csv\n",
      "salvo il file in csv THELASTOFUS.csv\n"
     ]
    }
   ],
   "source": [
    "# Importare le librerie\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import csv\n",
    "import datetime\n",
    "import locale\n",
    "\n",
    "urls =[{\n",
    "    \"link\": \"https://www.imdb.com/title/tt5180504/episodes/\",\n",
    "    \"csv\" : \"THEWITCHER.csv\"\n",
    "    },{\n",
    "    \"link\": \"https://www.imdb.com/title/tt3581920/episodes/\",\n",
    "    \"csv\" : \"THELASTOFUS.csv\"\n",
    "}]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"accept-language\": \"it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7\" \n",
    "}\n",
    "image_class = \"srcset\"\n",
    "\n",
    "\n",
    "def get_data(soup_episode):\n",
    "    data_tag = soup_episode.find(\"span\",class_=\"sc-f2169d65-10 iZXnmI\")\n",
    "    if data_tag != None:\n",
    "        return formatta_data (data_tag.text.strip())\n",
    "    return None\n",
    "\n",
    "def formatta_data(data_in):\n",
    "    data_splitted = data_in.split(\",\")[-1] \n",
    "    locale.setlocale(locale.LC_TIME, \"it_IT\")\n",
    "    data = datetime.datetime.strptime(data_splitted, \" %d %b %Y\")\n",
    "    data_out = data.strftime(\"%d/%m/%Y\")\n",
    "    return data_out\n",
    "\n",
    "def get_tot_value(soup_episode):\n",
    "    tot_value_tag = soup_episode.find(\"span\",class_=\"ipc-rating-star--voteCount\")\n",
    "    if tot_value_tag != None:\n",
    "        value=tot_value_tag.text.strip()\n",
    "        value_clean=value.replace(\"(\",\"\").replace(\")\",\"\").replace(\".\",\"\")\n",
    "        return int(value_clean)\n",
    "    return None\n",
    "def get_rank(soup_episode):\n",
    "    rank_tag = soup_episode.find(\"span\",class_=\"ipc-rating-star\")\n",
    "    if rank_tag != None:\n",
    "        rank=rank_tag.text.strip()\n",
    "        rank_clean=rank.split(\"/\")\n",
    "        if len(rank_clean) > 1:\n",
    "            rank_float=rank_clean[0].replace(\",\",\".\")\n",
    "            return  float(rank_float)\n",
    "        else:\n",
    "            rank_float=rank.replace(\",\",\".\")\n",
    "            return  float(rank_float)\n",
    "    return None\n",
    "    \n",
    "def get_identifier_title(soup_episode):\n",
    "    title_tag =soup_episode.find(\"div\",class_=\"ipc-title__text\")\n",
    "    if title_tag != None:\n",
    "        title = title_tag.text.strip()\n",
    "        title_splitted = title.split(\" âˆ™ \")\n",
    "        if len (title_splitted)>1:\n",
    "            return title_splitted[0],title_splitted[1]\n",
    "        else:\n",
    "            return title,None\n",
    "    return None,None\n",
    "                                 \n",
    "def get_last_link(input_string):\n",
    "    link_list = input_string.split(\", \")\n",
    "    last_link = link_list[-1]\n",
    "    last_link_split = last_link.split(\" \")\n",
    "    return last_link_split [0]\n",
    "\n",
    "def getImage_episode(soup_episode):\n",
    "    image_tag = soup_episode.find(\"img\")\n",
    "    if image_tag and image_class in image_tag.attrs:\n",
    "        image_list = image_tag[image_class]\n",
    "        return get_last_link(image_list)\n",
    "    \n",
    "def dictepisode(image,identifier,title,tot_value,data,rank):\n",
    "    episodes_info ={\n",
    "        \"Image\":image,\n",
    "        \"Identifier\":identifier,\n",
    "        \"Title\":title,\n",
    "        \"Tot_value\":tot_value,\n",
    "        \"Data\":data,\n",
    "        \"Rank\":rank\n",
    "    }\n",
    "    return episodes_info\n",
    "\n",
    "def into_csv(array, nome_file):\n",
    "    with open(nome_file, \"w\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=array[0].keys())\n",
    "        writer.writeheader()\n",
    "        for dizionario in array:\n",
    "            writer.writerow(dizionario)\n",
    "            \n",
    "for url in urls :\n",
    "    episodes=[]\n",
    "    response = requests.get(url[\"link\"], headers=headers)\n",
    "    if response.status_code ==200:\n",
    "        soup = bs(response.content,\"html.parser\")\n",
    "        cointainer = soup.find(\"section\",class_=\"sc-7b9ed960-0 jNjsLo\")\n",
    "        episode_list = cointainer.find_all(\"article\",class_=\"sc-282bae8e-1 dSEzwa episode-item-wrapper\")\n",
    "        for episode in episode_list:\n",
    "            image = getImage_episode(episode)\n",
    "            identifier,title = get_identifier_title(episode)\n",
    "            tot_value = get_tot_value(episode)\n",
    "            data = get_data(episode)\n",
    "            rank = get_rank(episode)\n",
    "            dictionary= dictepisode(image,identifier,title,tot_value,data,rank)\n",
    "            episodes.append(dictionary)\n",
    "    \n",
    "        into_csv(episodes,url[\"csv\"])\n",
    "        print(\"salvo il file in csv \"+ url[\"csv\"])\n",
    "    else:\n",
    "        print (response)\n",
    "        print(\"non sono riuscito a recuperare la pagina\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50156f65-cf1e-4c88-8db9-7516bbfe7f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
